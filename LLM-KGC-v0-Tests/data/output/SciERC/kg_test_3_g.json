{
  "iri": "Paper-3",
  "title": "INTERSPEECH_2013_21_abs",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-3-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-3-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-1",
              "text": "This work proposes a new research direction to address the lack of structures in traditional n-gram models ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-2",
              "text": "It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-3",
              "text": "Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-4",
              "text": "Bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-5",
              "text": "This posterior encodes sparse se-lectional preferences between a head word and its dependents ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-6",
              "text": "The model is evaluated on English and Czech newspaper texts , and is then validated on French broadcast news transcriptions ."
            }
          ]
        }
      ]
    }
  ],
  "summary": "This work proposes a new research direction to address the lack of structures in traditional n-gram models . It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus . Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge . Bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus . This posterior encodes sparse se-lectional preferences between a head word and its dependents . The model is evaluated on English and Czech newspaper texts , and is then validated on French broadcast news transcriptions .",
  "kg2text": [
    "This work proposes a new research direction that aims to enhance traditional n-gram models by introducing a weakly supervised dependency parser. This new research direction addresses the lack of structures in traditional n-gram models, which limits their ability to capture complex linguistic patterns. The model, which is evaluated on English and Czech newspaper texts, utilizes complex tree structures created through Bayesian inference, allowing it to effectively model speech syntax without relying on an annotated training corpus. Additionally, the weakly supervised dependency parser is based on hand-crafted rules that encode basic syntactic knowledge, further improving its performance. Overall, this work introduces a significant advancement in the field of natural language processing by proposing a model that overcomes the limitations of traditional n-gram models.",
    "Hand-crafted rules are a specific type of guidelines that fall under the broader category of basic syntactic knowledge, which encompasses fundamental linguistic rules. These hand-crafted rules are essential in understanding syntactic knowledge as they govern the behavior of systems in language processing. In the context of dependency grammar, a head word is crucial as it has its dependents, which are the words that modify or relate to it. The term 'its dependents' is a more specific reference to the dependents of a head word. Furthermore, English and Czech newspaper texts serve as a broader category for newspaper texts, providing a rich source of data for linguistic analysis. In machine learning contexts, labeled data can be replaced by hand-crafted rules, highlighting the importance of expert knowledge in developing effective language models."
  ],
  "times": [
    3.7889888286590576
  ]
}