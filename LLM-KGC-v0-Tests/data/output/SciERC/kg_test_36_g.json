{
  "iri": "Paper-36",
  "title": "C04-1011",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-36-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-36-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-36-Section-1-Paragraph-1-Sentence-1",
              "text": "We consider the problem of computing the Kullback-Leibler distance , also called the relative entropy , between a probabilistic context-free grammar and a probabilistic finite automaton ."
            },
            {
              "iri": "Paper-36-Section-1-Paragraph-1-Sentence-2",
              "text": "We show that there is a closed-form -LRB- analytical -RRB- solution for one part of the Kullback-Leibler distance , viz the cross-entropy ."
            },
            {
              "iri": "Paper-36-Section-1-Paragraph-1-Sentence-3",
              "text": "We discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata ."
            }
          ]
        }
      ]
    }
  ],
  "summary": "We consider the problem of computing the Kullback-Leibler distance , also called the relative entropy , between a probabilistic context-free grammar and a probabilistic finite automaton . We show that there is a closed-form -LRB- analytical -RRB- solution for one part of the Kullback-Leibler distance , viz the cross-entropy . We discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata .",
  "kg2text": [
    "The result applies to the problem of distributional approximation, which involves effectively representing probabilistic context-free grammars using probabilistic finite automata. This result has applications in various practical contexts, highlighting its significance. In the realm of information theory, the Kullback-Leibler distance, also known as relative entropy, measures the divergence between probabilistic context-free grammars and probabilistic finite automata. This distance incorporates the concept of cross-entropy, which is a crucial component in its calculation. Specifically, cross-entropy has an analytical solution that can be expressed in a closed-form, providing a means to quantify the differences between these two types of probabilistic models. Furthermore, probabilistic context-free grammars are approximated by probabilistic finite automata, demonstrating the interplay between these mathematical constructs."
  ],
  "times": [
    2.1497581005096436
  ]
}