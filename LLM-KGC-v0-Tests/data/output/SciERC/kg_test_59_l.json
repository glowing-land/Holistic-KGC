{
  "iri": "Paper-59",
  "title": "P05-1046",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-59-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-59-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-59-Section-1-Paragraph-1-Sentence-1",
              "text": "The applicability of many current information extraction techniques is severely limited by the need for supervised training data ."
            },
            {
              "iri": "Paper-59-Section-1-Paragraph-1-Sentence-2",
              "text": "We demonstrate that for certain field structured extraction tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion ."
            },
            {
              "iri": "Paper-59-Section-1-Paragraph-1-Sentence-3",
              "text": "Although hidden Markov models -LRB- HMMs -RRB- provide a suitable generative model for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains ."
            },
            {
              "iri": "Paper-59-Section-1-Paragraph-1-Sentence-4",
              "text": "However , one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions ."
            },
            {
              "iri": "Paper-59-Section-1-Paragraph-1-Sentence-5",
              "text": "In both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data ."
            }
          ]
        }
      ]
    }
  ],
  "summary": "The applicability of many current information extraction techniques is severely limited by the need for supervised training data . We demonstrate that for certain field structured extraction tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion . Although hidden Markov models -LRB- HMMs -RRB- provide a suitable generative model for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains . However , one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions . In both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .",
  "kg2text": [
    "Many current information extraction techniques are severely limited by the need for supervised training data. However, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion, which has broader implications for unsupervised methods that can attain accuracies comparable to those attained by supervised methods. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, they are not the only approach available. The applicability of these techniques is limited by the need for supervised training data, which highlights the importance of simple prior knowledge of desired solutions that can be exploited to improve the learned structure. We demonstrate this concept through certain field-structured extraction tasks involving classified advertisements and bibliographic citations."
  ],
  "times": [
    6.588482856750488
  ]
}