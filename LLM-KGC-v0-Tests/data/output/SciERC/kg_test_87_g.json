{
  "iri": "Paper-87",
  "title": "N03-1033",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-87-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-87-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-87-Section-1-Paragraph-1-Sentence-1",
              "text": "We present a new part-of-speech tagger that demonstrates the following ideas : -LRB- i -RRB- explicit use of both preceding and following tag contexts via a dependency network representation , -LRB- ii -RRB- broad use of lexical features , including jointly conditioning on multiple consecutive words , -LRB- iii -RRB- effective use of priors in conditional loglinear models , and -LRB- iv -RRB- fine-grained modeling of unknown word features ."
            },
            {
              "iri": "Paper-87-Section-1-Paragraph-1-Sentence-2",
              "text": "Using these ideas together , the resulting tagger gives a 97.24 % accuracy on the Penn Treebank WSJ , an error reduction of 4.4 % on the best previous single automatically learned tagging result ."
            }
          ]
        }
      ]
    }
  ],
  "summary": "We present a new part-of-speech tagger that demonstrates the following ideas : -LRB- i -RRB- explicit use of both preceding and following tag contexts via a dependency network representation , -LRB- ii -RRB- broad use of lexical features , including jointly conditioning on multiple consecutive words , -LRB- iii -RRB- effective use of priors in conditional loglinear models , and -LRB- iv -RRB- fine-grained modeling of unknown word features . Using these ideas together , the resulting tagger gives a 97.24 % accuracy on the Penn Treebank WSJ , an error reduction of 4.4 % on the best previous single automatically learned tagging result .",
  "kg2text": [
    "The resulting tagger achieves a remarkable 97.24% accuracy, surpassing the best previous single automatically learned tagging result. This new part-of-speech tagger demonstrates several advanced features, including a dependency network representation, which enhances its contextual understanding, and the incorporation of unknown word features, allowing it to effectively handle words not present in the training data. Additionally, it utilizes conditional loglinear models and broad lexical features to further improve its performance. The resulting tagger not only provides an error reduction of 4.4% compared to its predecessor but also exemplifies the broader category of part-of-speech taggers."
  ],
  "times": [
    2.8677659034729004
  ]
}