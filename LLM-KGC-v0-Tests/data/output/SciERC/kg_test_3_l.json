{
  "iri": "Paper-3",
  "title": "INTERSPEECH_2013_21_abs",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-3-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-3-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-1",
              "text": "This work proposes a new research direction to address the lack of structures in traditional n-gram models ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-2",
              "text": "It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-3",
              "text": "Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-4",
              "text": "Bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-5",
              "text": "This posterior encodes sparse se-lectional preferences between a head word and its dependents ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-6",
              "text": "The model is evaluated on English and Czech newspaper texts , and is then validated on French broadcast news transcriptions ."
            }
          ]
        }
      ]
    }
  ],
  "summary": "This work proposes a new research direction to address the lack of structures in traditional n-gram models . It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus . Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge . Bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus . This posterior encodes sparse se-lectional preferences between a head word and its dependents . The model is evaluated on English and Czech newspaper texts , and is then validated on French broadcast news transcriptions .",
  "kg2text": [
    "This work proposes a new research direction that can model speech syntax without relying on any annotated training corpus. This approach addresses the lack of structures in traditional n-gram models, which has been a limitation inherent in classical n-gram-based approaches. The proposed methodology uses weakly supervised dependency parsers and Bayesian inference to create complex tree structures. It is based on a few hand-crafted rules that encode basic syntactic knowledge, replacing labeled data requirements. This new research direction can model speech syntax without relying on any annotated training corpus."
  ],
  "times": [
    7.090601682662964
  ]
}