{
  "iri": "Paper-42",
  "title": "H01-1042",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-42-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-42-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-1",
              "text": "The purpose of this research is to test the efficacy of applying automated evaluation techniques , originally devised for the evaluation of human language learners , to the output of machine translation -LRB- MT -RRB- systems ."
            },
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-2",
              "text": "We believe that these evaluation techniques will provide information about both the human language learning process , the translation process and the development of machine translation systems ."
            },
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-3",
              "text": "This , the first experiment in a series of experiments , looks at the intelligibility of MT output ."
            },
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-4",
              "text": "A language learning experiment showed that assessors can differentiate native from non-native language essays in less than 100 words ."
            },
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-5",
              "text": "Even more illuminating was the factors on which the assessors made their decisions ."
            },
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-6",
              "text": "We tested this to see if similar criteria could be elicited from duplicating the experiment using machine translation output ."
            },
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-7",
              "text": "Subjects were given a set of up to six extracts of translated newswire text ."
            },
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-8",
              "text": "Some of the extracts were expert human translations , others were machine translation outputs ."
            },
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-9",
              "text": "The subjects were given three minutes per extract to determine whether they believed the sample output to be an expert human translation or a machine translation ."
            },
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-10",
              "text": "Additionally , they were asked to mark the word at which they made this decision ."
            },
            {
              "iri": "Paper-42-Section-1-Paragraph-1-Sentence-11",
              "text": "The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here ."
            }
          ]
        }
      ]
    }
  ],
  "summary": "The purpose of this research is to test the efficacy of applying automated evaluation techniques , originally devised for the evaluation of human language learners , to the output of machine translation -LRB- MT -RRB- systems . We believe that these evaluation techniques will provide information about both the human language learning process , the translation process and the development of machine translation systems . This , the first experiment in a series of experiments , looks at the intelligibility of MT output . A language learning experiment showed that assessors can differentiate native from non-native language essays in less than 100 words . Even more illuminating was the factors on which the assessors made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using machine translation output . Subjects were given a set of up to six extracts of translated newswire text . Some of the extracts were expert human translations , others were machine translation outputs . The subjects were given three minutes per extract to determine whether they believed the sample output to be an expert human translation or a machine translation . Additionally , they were asked to mark the word at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .",
  "kg2text": [
    "This research supports the results of this experiment, which evaluates machine translation (MT) systems. The results of this experiment will be presented alongside a preliminary analysis of the factors involved in the decision-making process, as well as the specific factors that assessors considered when making their decisions. The first experiment in a series of experiments looks at the intelligibility of MT output, utilizing extracts that consist of translated newswire text. Automated evaluation techniques are tested for their efficacy in assessing the output of machine translation systems. Assessors made decisions based on their ability to differentiate between non-native language essays and machine translation outputs. Furthermore, the sample output is identified as a machine translation, which is part of the broader context of the development of machine translation systems.",
    "The first experiment in a series of experiments elicits similar criteria that assessors use to evaluate the intelligibility of machine translation outputs. These assessors can differentiate between native from non-native language essays based on specific factors. In this study, subjects were given extracts of translated newswire text and tasked with determining whether they believed the sample output to be expert human translations. A preliminary analysis of the factors involved in the decision-making process has a broader term, which encompasses the various factors that influence assessors' evaluations. The language learning experiment showed that assessors play a crucial role in this evaluation process. Additionally, subjects were allocated three minutes per extract to make their assessments. Automated evaluation techniques provide valuable information about the development of machine translation systems and the translation process, originally devised for the evaluation of human language learners. Overall, the results of this experiment contribute to a broader understanding of the experiment itself and the intricacies of machine translation.",
    "Machine translation encompasses the development of machine translation systems, which refers to the creation and enhancement of automated translation processes. Within this domain, a machine translation is part of the broader translation process, which also includes expert human translations. The intelligibility of MT output, a key aspect of evaluating machine translations, falls under the general concept of intelligibility. The first experiment in a series of experiments specifically looks at intelligibility, aiming to assess how well machine translation outputs are understood compared to human translations. This research, which is a subset of broader research efforts, utilizes automated evaluation techniques that are categorized under evaluation techniques. Furthermore, the factors on which the assessors made their decisions are integral to the decision making process, which is influenced by a preliminary analysis of these factors. This analysis helps to clarify the criteria used to differentiate between machine translations and expert human translations. Additionally, the human language learning process is relevant to human language learners, who are the focus of language learning experiments that also fall under the broader category of experiments."
  ],
  "times": [
    9.933537721633911
  ]
}