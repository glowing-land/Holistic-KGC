{
  "iri": "Paper-MEL_Metadata_Extractor__Loader",
  "title": "MEL: Metadata Extractor & Loader",
  "authors": [
    "Sergio J. Rodr\u00edguez M\u00e9ndez",
    "Pouya G. Omran",
    "Armin Haller",
    "KerryTaylor"
  ],
  "keywords": [
    "Metadata Extraction",
    "Information Extraction",
    "Data Preprocessing",
    "Knowledge Graph Construction",
    "Data Analysis Pipeline"
  ],
  "sections": [
    {
      "iri": "Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-1-Paragraph-1-Sentence-1",
              "text": "The metadata and content-based information extraction tasks from heterogeneous file sets are pre-processing steps of many Knowledge Graph Construction Pipelines (KGCP)."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-2",
              "text": "These tasks often take longer than necessary due to the lack of proper tools that integrate several complementary extraction methods and properties to get a rich output set."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-3",
              "text": "This paper presents MEL, a Python-based tool that implements a set of methods to extract metadata and content-based information from unstructured information encoded in different source document formats."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-4",
              "text": "The results are generated as JSON files, which can: (a) optionally be stored in a document store, and (b) easily be mapped to RDF using a variety of tools such as J2RM."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-5",
              "text": "MEL supports more than 20 different file types, making it a versatile tool that aids pre-processing tasks as part of a KGCP based on comprehensive configurable settings."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-2",
      "subtitle": "Introduction",
      "paragraphs": [
        {
          "iri": "Section-2-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-1-Sentence-1",
              "text": "This paper introduces MEL, a tool that implements a set of methods to extract metadata and content-based information from various file formats as JSON objects."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-2",
              "text": "For each supported file type, MEL extracts the textual content from the source document and performs specific pre-processing and data cleaning tasks."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-3",
              "text": "Also, it performs basic text analysis tasks (pattern matching and keyword extraction) and generates the results in a machine-readable format (JSON), preparing the ground for content-based analysis."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-4",
              "text": "MEL is integrated with \u201cThe NLP -NER Toolkit\u201d (TNNT), which automates the extraction task of categorised named entities from the MEL results by using diverse state-of-the-art NLP tools and NER models [5]."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-5",
              "text": "MEL implements primitives for metadata and content extraction from unstructured data sets of heterogeneous formats, and along with the TNNT results, it provides the groundwork for content-based analysis."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-6",
              "text": "MEL and TNNT were developed in conjunction with J2RM [4], to easily map the JSON results to RDF as part of an automated KGCP."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-3",
      "subtitle": "Core Features",
      "paragraphs": [
        {
          "iri": "Section-3-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-1-Sentence-1",
              "text": "MEL has comprehensive metadata extraction support of various file types and formats."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-2",
              "text": "In a nutshell: (1) it takes as input a document (file) set; (2) then, for each document, it extracts its related metadata and content-based information, while performing basic text analysis (such as applying a configurable set of regular expressions and keyword extraction task); and, (3) as output, it generates a JSON file with the extracted metadata and text content with a structure based on the supported formats' document object model."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-3",
              "text": "It can store the results in a document store."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-4",
              "text": "MEL's general output structure is presented in Table 1."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-5",
              "text": "MEL has a detailed configuration JSON file that defines how the processing will be performed through a set of parameters and flags that establish the initial settings related to the document store, input document sets, TNNT general configuration, file extension mappings, the \u201cAssociated-Metadata\u201d processing (Table 1), and regular expressions to apply in the text analysis task, among other settings."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-6",
              "text": "The supported file types are presented in Table 2."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-7",
              "text": "The third column shows the theoretical number of attributes that the tool is able to extract per document type, whilst the fourth column shows the average of the extracted attributes from four use case document sets."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-8",
              "text": "OLE 2 file types and .docm can only be processed on Windows operating systems."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-9",
              "text": "Specifically for OLE 2 file types, MEL uses the olemeta tool."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-4",
      "subtitle": "Architecture",
      "paragraphs": [
        {
          "iri": "Section-4-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-4-Paragraph-1-Sentence-1",
              "text": "MEL is fully integrated with TNNT as depicted in Figure 1."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-2",
              "text": "The set of Python-based methods implemented in MEL are generic and can be applied to extract the content and metadata of all supported file types."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-3",
              "text": "MEL uses various opensource packages and tools with complementary capabilities to form a \u201cSwiss army knife\u201d of metadata and content-based information extraction from heterogeneous document sets."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-4",
              "text": "As part of the \u201cGeneral-Metadata\u201d extraction task, MEL optionally uses the XML output from the NLNZ Metadata Extractor tool, a Java standalone tool that extracts a comprehensive attribute and property list from dozens of file formats."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-5",
              "text": "The MEL general processing model is presented in Figure 2."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-6",
              "text": "It is important to note that each file type has its own specific processing model as well as the text analysis task, which is the last step that is performed for any output."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-5",
      "subtitle": "Related Work",
      "paragraphs": [
        {
          "iri": "Section-5-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-5-Paragraph-1-Sentence-1",
              "text": "The most comprehensive and current state-of-the-art tool for content extraction and analysis is Apache Tika, which is a complete and complex Java-based general-purpose system."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-2",
              "text": "While MEL's core goals resemble the ones of Apache Tika, the main difference and benefit of MEL as compared to Apache Tika is that it is a lightweight Python-based package for the metadata extraction of common file formats aimed to be used in a KGCP."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-3",
              "text": "Although there is a wide range of Python-based tools and libraries for metadata extraction, to the best of our knowledge, there is no package available that fully integrates in one system a comprehensive set of methods for metadata and content extraction of common file formats that generate the results in JSON structures based on the document object model of each format type."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-4",
              "text": "Last, MEL can assist in the information extraction stage of several KGCPs, such as the ones described in [6], [2], and [3]."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-6",
      "subtitle": "Conclusions and Future Work",
      "paragraphs": [
        {
          "iri": "Section-6-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-1-Sentence-1",
              "text": "MEL provides a versatile mechanism to extract metadata and content-based information from unstructured data sets of heterogeneous file formats, agnostic of the data sets' domain (general purpose)."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-2",
              "text": "It has been tested over thousands of documents using different formats and datasets as part of the AGRIF project."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-3",
              "text": "Based on the structure of the MEL's JSON results, it is possible to easily add a vocabulary or light-weight ontology using JSON-LD annotations, in order to make the extracted metadata \u201cRDF ready\u201d."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-4",
              "text": "This will be explored in the near future leveraging on the integration with JSON-LD ontologies."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-5",
              "text": "More file formats will be added in a per use-case requirements basis, in order to support KGCP tasks."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-6",
              "text": "Additionally, a project to \u201ccontainerise\u201d the MEL+TNNT tools is planned in the near future."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-7",
              "text": "The major contributions of this tool are: (1) the ability to extract metadata sets and content-based information from different source document formats; (2) the comprehensive support of over 20 different file types/formats integrated into one easy-to-use Python-based system; (3) integration with TNNT which automates the extraction of categorised named entities from the results by using diverse state-of-the-art NLP tools and NER models; and (4) the JSON result files can be easily mapped to RDF using J2RM."
            }
          ]
        }
      ]
    }
  ]
}